# Prometheus Alert Rules for LLM Auto Optimizer

groups:
  - name: llm_optimizer_alerts
    interval: 30s
    rules:
      # Service availability
      - alert: ServiceDown
        expr: up{job="llm-optimizer"} == 0
        for: 2m
        labels:
          severity: critical
          component: service
        annotations:
          summary: "LLM Optimizer service is down"
          description: "Service {{ $labels.instance }} has been down for more than 2 minutes."

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          component: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      # Performance
      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
        for: 10m
        labels:
          severity: warning
          component: performance
        annotations:
          summary: "High request latency"
          description: "95th percentile latency is {{ $value }}s on {{ $labels.instance }}"

      - alert: SlowOptimizationCycle
        expr: optimization_cycle_duration_seconds > 600
        for: 5m
        labels:
          severity: warning
          component: optimizer
        annotations:
          summary: "Optimization cycle is slow"
          description: "Optimization cycle taking {{ $value }}s (target: <300s)"

      # Resource usage
      - alert: HighCPUUsage
        expr: rate(process_cpu_seconds_total[5m]) > 0.8
        for: 15m
        labels:
          severity: warning
          component: resources
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 / 1024 > 3.5
        for: 10m
        labels:
          severity: warning
          component: resources
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize }}GB on {{ $labels.instance }}"

      - alert: MemoryLeakSuspected
        expr: rate(process_resident_memory_bytes[1h]) > 10485760
        for: 2h
        labels:
          severity: warning
          component: resources
        annotations:
          summary: "Possible memory leak"
          description: "Memory usage increasing by {{ $value | humanize }}B/s over 2 hours"

      # Database
      - alert: DatabaseConnectionPoolExhausted
        expr: db_connections_active / db_connections_max > 0.9
        for: 5m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "{{ $value | humanizePercentage }} of database connections in use"

      - alert: SlowDatabaseQueries
        expr: histogram_quantile(0.95, rate(db_query_duration_seconds_bucket[5m])) > 1
        for: 10m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Slow database queries"
          description: "95th percentile query time is {{ $value }}s"

      # Redis
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 2m
        labels:
          severity: critical
          component: cache
        annotations:
          summary: "Redis is down"
          description: "Redis instance {{ $labels.instance }} is unreachable"

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "Redis memory usage high"
          description: "Redis using {{ $value | humanizePercentage }} of available memory"

      - alert: RedisHighEvictionRate
        expr: rate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
          component: cache
        annotations:
          summary: "High Redis key eviction rate"
          description: "Redis evicting {{ $value }} keys/second"

      # Optimization metrics
      - alert: HighCostIncrease
        expr: rate(optimization_cost_total[1h]) > 100
        for: 15m
        labels:
          severity: warning
          component: cost
        annotations:
          summary: "Significant cost increase detected"
          description: "Cost increasing by ${{ $value }}/hour"

      - alert: QualityDegradation
        expr: avg(optimization_quality_score) < 0.7
        for: 30m
        labels:
          severity: warning
          component: quality
        annotations:
          summary: "Quality score degradation"
          description: "Average quality score dropped to {{ $value }}"

      - alert: HighDrift
        expr: drift_score > 0.2
        for: 15m
        labels:
          severity: warning
          component: drift
        annotations:
          summary: "High drift detected"
          description: "Drift score is {{ $value }} (threshold: 0.1)"

      - alert: FailedCanaryDeployment
        expr: canary_deployment_status{status="failed"} > 0
        for: 1m
        labels:
          severity: critical
          component: deployment
        annotations:
          summary: "Canary deployment failed"
          description: "Canary deployment {{ $labels.deployment_id }} has failed"

      # System health
      - alert: TooManyRestarts
        expr: changes(process_start_time_seconds[1h]) > 3
        for: 5m
        labels:
          severity: critical
          component: stability
        annotations:
          summary: "Service restarting frequently"
          description: "Service has restarted {{ $value }} times in the last hour"

      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) < 0.1
        for: 10m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "Low disk space"
          description: "Only {{ $value | humanizePercentage }} disk space remaining"

  - name: postgresql_alerts
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL instance {{ $labels.instance }} is unreachable"

      - alert: PostgreSQLTooManyConnections
        expr: sum(pg_stat_activity_count) > 180
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Too many PostgreSQL connections"
          description: "PostgreSQL has {{ $value }} connections (max: 200)"

      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag_seconds > 60
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "PostgreSQL replication lag"
          description: "Replication lag is {{ $value }}s on {{ $labels.instance }}"
